{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b763cc-ead3-432a-a68a-641c909789ee",
   "metadata": {},
   "source": [
    "![spacecraft with bounding box](https://drivendata-public-assets.s3.amazonaws.com/nasa-spacecraft-leading-example.jpg)\n",
    "\n",
    "Welcome to the benchmark notebook for the **Pose Bowl: Object Detection** challenge!\n",
    "\n",
    "If you are just getting started, first checkout the competition [homepage](https://www.drivendata.org/competitions/260/spacecraft-detection/) and [problem description](https://www.drivendata.org/competitions/260/spacecraft-detection/page/833/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf773b15-b7af-4a44-a129-cc6be292be41",
   "metadata": {},
   "source": [
    "## Pose Bowl: Object Detection\n",
    "\n",
    "In this challenge, you will help to develop new methods for conducting spacecraft inspections by identifying the position and orientation of a target spacecraft in an image.\n",
    "\n",
    "The goal of this $12,000 challenge is to help NASA address key operational hurdles in spacecraft inspection, including the ability to detect a variety of different types of target spacecraft, as well as being able to run this code efficiently on relatively low-cost hardware. For the latter reason, this challenge is a **code execution challenge**, and your submission will be run on our code execution platform in an environment that simulates a small, [off-the-shelf computer board](https://www.aaeon.com/en/p/up-board-computer-board-for-professional-makers) used on [R5 NASA spacecraft](https://ntrs.nasa.gov/api/citations/20230011368/downloads/R5_Fact_Sheet_2023-07.pdf).\n",
    "\n",
    "If you're not familiar with code execution challenges, don't worry! This benchmark notebook/blog post will help you get comfortable with the setup and on your way to producing your own challenge submissions.\n",
    "\n",
    "We'll cover two main areas in this post:\n",
    "\n",
    "- [**Section 1. Getting started:**](#getting-started) An introduction to the challenge data, including image examples and some explanation about the type of variation to expect across the dataset.\n",
    "\n",
    "- [**Section 2. Demo submission:**](#demo-submission) A demonstration of how to run the benchmark example and produce a valid code submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c626ad9-d5d7-4e72-a36d-15d5d5fdcbc1",
   "metadata": {},
   "source": [
    "<a id=\"getting-started\"></a>\n",
    "\n",
    "## Section 1: Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d732a37-a0ae-43cb-9ac2-f3afcc632c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get this notebook\n",
    "\n",
    "First, clone the runtime repository with:\n",
    "```\n",
    "git clone https://github.com/drivendataorg/spacecraft-pose-object-detection-runtime.git\n",
    "```\n",
    "\n",
    "The repository includes a copy of this notebook in `/notebooks/data-exploration-and-benchmark.ipynb`. We'd encourage you to run this notebook yourself as part of this exercise.\n",
    "\n",
    "You will also need to set up your environment. For the purposes of this notebook, you will just need to the following libraries, which you can install with the following `pip` command.\n",
    "```\n",
    "pip install jupyterlab pandas pyarrow opencv-python matplotlib ultralytics\n",
    "```\n",
    "\n",
    "### Download some data\n",
    "\n",
    "Let's first download some challenge imagery from the [Data Download](https://www.drivendata.org/competitions/260/spacecraft-detection/data/) page. The imagery dataset comes in the form of `.tar` files, each of which is about 2.5 GB. For this benchmark example, we'll just download the `0.tar` file.\n",
    "\n",
    "Downloading imagery may take some time, so feel free to do something else for a little while. (The [code submission format page](https://www.drivendata.org/competitions/260/spacecraft-detection/page/835/) and this runtime repository's [README](https://github.com/drivendataorg/spacecraft-pose-object-detection-runtime) are relevant background reading for what we're about to do next.)\n",
    "\n",
    "Once the tar file has been downloaded, you can extract it to the `data_dev` directory (the command below should work on a Unix-based system). We suggest saving the images to the `data_dev` directory (or something similarly named) rather than the `data` directory because the `data` directory plays a special role in simulating the test data that is available when your submission runs in the code execution environment.\n",
    "```\n",
    "tar -xzvf 0.tar -C data_dev/\n",
    "```\n",
    "You'll also want to download the `submission_format.csv`, `train_labels.csv` and `train_metadata.csv` from the [Data Download](https://www.drivendata.org/competitions/260/spacecraft-detection/data/) page and save these in the same `data_dev` directory.\n",
    "\n",
    "Once everything is downloaded, your `data_dev` directory should look like this.\n",
    "```\n",
    "spacecraft-pose-object-detection-runtime/\n",
    "└── data_dev\n",
    "    ├── images\n",
    "    │   ├── 0001954c9f4a58f7ac05358b3cda8d20.png\n",
    "    │   ├── 00054819240f9d46378288b215dbcd3a.png\n",
    "    │   ├── 000dbf763348037b46558bbcb6a032ac.png\n",
    "    │   ...\n",
    "    │\n",
    "    ├── submission_format.csv\n",
    "    ├── train_labels.csv\n",
    "    └── train_metadata.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50283cd-09fe-496f-8f0e-03bdea2a0564",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "\n",
    "Now it's time to get acquainted with the challenge data. Below we define locations for some of the important files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7447df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:06.475436Z",
     "start_time": "2024-03-05T22:20:06.347224Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJ_DIRECTORY = Path.cwd().parent\n",
    "DATA_DIRECTORY = PROJ_DIRECTORY / \"data\"\n",
    "DEV_DIRECTORY = PROJ_DIRECTORY / \"data_dev\"\n",
    "IMAGES_DIRECTORY = DEV_DIRECTORY / \"images\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c954c",
   "metadata": {},
   "source": [
    "Let's take a look at two of the metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0588c0e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:06.605037Z",
     "start_time": "2024-03-05T22:20:06.480289Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(DEV_DIRECTORY / \"train_metadata.csv\", index_col=\"image_id\")\n",
    "train_labels = pd.read_csv(DEV_DIRECTORY / \"train_labels.csv\", index_col=\"image_id\")\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16724d83",
   "metadata": {},
   "source": [
    "The `train_metadata.csv` contains information about the type of spacecraft and background used to generate the image. You should consider ways to use this information effectively in stratifying your train and test splits. The test data that you will be evaluated on includes spacecraft types that are not included in the training set, so you will want to consider strategies for ensuring your model generalizes well. You may also want to consider ways of generating additional training data, e.g. to generate a more diverse variety of background imagery.\n",
    "\n",
    "The `train_metadata.csv` contains information about all images in the training set. But since we haven't downloaded all the images yet, let's filter the dataset down to just the images we've saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edcd7bb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:07.047615Z",
     "start_time": "2024-03-05T22:20:06.569740Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacecraft_id</th>\n",
       "      <th>background_id</th>\n",
       "      <th>exists</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100178090f6002cbd81d34896ab826e4</th>\n",
       "      <td>6</td>\n",
       "      <td>157</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002ee52da534ef4d5495f1b1fe463f7</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004dd5281b1c2086c9e62862bbb8515</th>\n",
       "      <td>11</td>\n",
       "      <td>168</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005515249a26a99d79d9db5025d3c66</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006e66f71ae40eb1409da105dc5642c</th>\n",
       "      <td>28</td>\n",
       "      <td>194</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  spacecraft_id  background_id  exists\n",
       "image_id                                                              \n",
       "100178090f6002cbd81d34896ab826e4              6            157    True\n",
       "1002ee52da534ef4d5495f1b1fe463f7             18             10    True\n",
       "1004dd5281b1c2086c9e62862bbb8515             11            168    True\n",
       "1005515249a26a99d79d9db5025d3c66              5              8    True\n",
       "1006e66f71ae40eb1409da105dc5642c             28            194    True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we didn't download the full training set, so add a column indicating which images are saved locally\n",
    "train_meta[\"exists\"] = train_meta.index.to_series().map(lambda x: (IMAGES_DIRECTORY / f\"{x}.png\").exists())\n",
    "# filter our metadata down to only the images we have locally\n",
    "train_meta = train_meta[train_meta.exists]\n",
    "\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde759c4",
   "metadata": {},
   "source": [
    "The `train_labels.csv` contains the bounding box information for the target spacecraft in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15157ade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:07.062459Z",
     "start_time": "2024-03-05T22:20:07.024787Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001954c9f4a58f7ac05358b3cda8d20</th>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>345</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00054819240f9d46378288b215dbcd3a</th>\n",
       "      <td>753</td>\n",
       "      <td>602</td>\n",
       "      <td>932</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000dbf763348037b46558bbcb6a032ac</th>\n",
       "      <td>160</td>\n",
       "      <td>434</td>\n",
       "      <td>203</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000e79208bebd8e84ce6c22fd8612a0d</th>\n",
       "      <td>70</td>\n",
       "      <td>534</td>\n",
       "      <td>211</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000f13aff94499d03e3997afc55b0aa0</th>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  xmin  ymin  xmax  ymax\n",
       "image_id                                                \n",
       "0001954c9f4a58f7ac05358b3cda8d20     0   277   345   709\n",
       "00054819240f9d46378288b215dbcd3a   753   602   932   725\n",
       "000dbf763348037b46558bbcb6a032ac   160   434   203   481\n",
       "000e79208bebd8e84ce6c22fd8612a0d    70   534   211   586\n",
       "000f13aff94499d03e3997afc55b0aa0   103     0   312   193"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2676ddb2",
   "metadata": {},
   "source": [
    "Let's look at a few example images to get a feel for what's in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb495503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:09.999149Z",
     "start_time": "2024-03-05T22:20:07.040510Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.dnn' has no attribute 'DictValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rectangle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/cv2/__init__.py:175\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: binary extension... OK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m __collect_extra_submodules(DEBUG):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m__load_extra_py_code_for_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/cv2/__init__.py:28\u001b[0m, in \u001b[0;36m__load_extra_py_code_for_module\u001b[0;34m(base, name, enable_debug_print)\u001b[0m\n\u001b[1;32m     26\u001b[0m native_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(module_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     py_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m enable_debug_print:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/cv2/typing/__init__.py:168\u001b[0m\n\u001b[1;32m    166\u001b[0m ExtractArgsCallback \u001b[38;5;241m=\u001b[39m _typing\u001b[38;5;241m.\u001b[39mCallable[[_typing\u001b[38;5;241m.\u001b[39mSequence[GTypeInfo]], _typing\u001b[38;5;241m.\u001b[39mSequence[GRunArg]]\n\u001b[1;32m    167\u001b[0m ExtractMetaCallback \u001b[38;5;241m=\u001b[39m _typing\u001b[38;5;241m.\u001b[39mCallable[[_typing\u001b[38;5;241m.\u001b[39mSequence[GTypeInfo]], _typing\u001b[38;5;241m.\u001b[39mSequence[GMetaArg]]\n\u001b[0;32m--> 168\u001b[0m LayerId \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDictValue\u001b[49m\n\u001b[1;32m    169\u001b[0m IndexParams \u001b[38;5;241m=\u001b[39m _typing\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, _typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m    170\u001b[0m SearchParams \u001b[38;5;241m=\u001b[39m _typing\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, _typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2.dnn' has no attribute 'DictValue'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def get_bbox(image_id, labels):\n",
    "    \"\"\"Get bbox coordinates as list from dataframe for given image id.\"\"\"\n",
    "    return labels.loc[image_id].loc[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values.tolist()\n",
    "\n",
    "def display_image(image_id, images_dir=IMAGES_DIRECTORY, show_bbox=False, labels=train_labels):\n",
    "    \"\"\"Display image given image ID. Annotate with bounding box if `show_bbox` is True.\"\"\"\n",
    "    img = cv2.imread(str(images_dir / f\"{image_id}.png\"))\n",
    "    fig, ax = plt.subplots()\n",
    "    # cv2 reads images as BGR order; we should flip them to RGB for matplotlib\n",
    "    # ref: https://stackoverflow.com/questions/54959387/rgb-image-display-in-matplotlib-plt-imshow-returns-a-blue-image\n",
    "    ax.imshow(np.flip(img, axis=-1))\n",
    "\n",
    "    if show_bbox:\n",
    "        xmin, ymin, xmax, ymax = get_bbox(image_id, labels)\n",
    "        patch = Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, edgecolor='white', linewidth=1)\n",
    "        ax.add_patch(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ee3eaccb758b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4391406",
   "metadata": {},
   "source": [
    "Some images are relatively straightforward with an easily identifiable spacecraft. Below is one example. Note that the spacecraft is not necessarily fully in view. Also note that the background in this case consists entirely of our planet, with no view of outer space, while in other images it may be entirely space, or more commonly a mix of planet and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59697c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:10.018535Z",
     "start_time": "2024-03-05T22:20:10.002399Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_image(\"07dd7b5a0b7b224abc0a7fea8e78de76\", show_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7e547",
   "metadata": {},
   "source": [
    "There are many examples where the spacecraft will be more difficult to detect.\n",
    "\n",
    "One challenging type of image involves target spacecraft that appear very small, due to their distance from the chaser spacecraft. Here is one example of that situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276728e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.005712Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_image(\"0d4b4eda4bf7c0251399047d71cc4188\", show_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee37e1",
   "metadata": {},
   "source": [
    "Lens flare and other visual artifacts from the refraction of light on the camera lens may also complicate your detection algorithm, as in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a77731",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.007959Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_image(\"01fcd95cdcf8bb84ec4dfa7b87bf2abc\", show_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34d0bb",
   "metadata": {},
   "source": [
    "Some target spacecraft may have long, thin appendages that are difficult or even impossible to see. Remember that your bounding box needs to encompass these appendages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6fa7a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.010047Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_image(\"0263257179fcf14793f40430713ece44\", show_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f25267",
   "metadata": {},
   "source": [
    "Finally, there will be situations where parts of the spacecraft are obscured by shadow. This can make it particularly difficult to detect the edges of thin appendages like the ones we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae5ae4dfba00d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.012802Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_image(\"0eee2fcf59ec1e7641db1f8284c1d935\", show_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b8316",
   "metadata": {},
   "source": [
    "### Explore for yourself\n",
    "\n",
    "The code below will pull and display a random image. Feel free to run this as many times as you like to develop your intuition for what types of images are in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9aa27",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.018176Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_id = train_meta.sample(1).index[0]\n",
    "print(image_id)\n",
    "\n",
    "ax = display_image(image_id, show_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8db9ae",
   "metadata": {},
   "source": [
    "<a id=\"demo-submission\"></a>\n",
    "\n",
    "## Section 2: Demo submission\n",
    "Now that we've had a chance to get a feel for the data, it's time to walk through the steps for creating a competition submission.\n",
    "\n",
    "This is a **code submission competition**, so our focus for now will be on creating that submission in the correct format, and less so on the accuracy of the predictions. To get you started, we'll be using a YOLO model to try to identify the target spacecraft in 100 local test images.\n",
    "\n",
    "If you haven't already read through the following resources, now would be a great time to do that:\n",
    "* [Code Submission Format page](https://www.drivendata.org/competitions/260/spacecraft-detection/page/835/): An introduction to the code submission setup and how to make a valid submission.\n",
    "* [Runtime Repository README](https://github.com/drivendataorg/spacecraft-pose-object-detection-runtime): Details on the competition runtime and how to use this repository effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c85841",
   "metadata": {},
   "source": [
    "### Using the `/data` directory\n",
    "\n",
    "The `/data` directory in this runtime repository plays two important roles:\n",
    "\n",
    "* When you are testing your solution _locally_, the contents of `/data` will be mounted on the Docker container that simulates our code execution platform (further below, we'll walk through the specifics of how to do these test runs)\n",
    "* When you make a submission on the DrivenData _platform_, the contents of `/data` that get mounted to the Docker container will be the unseen test data that ultimately determines your rank on the leaderboard.\n",
    "\n",
    "To develop your submission locally, you should add a subset of the challenge data to `/data` and treat this as your local test set. We'll demonstrate one way to do that next..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea10dd1",
   "metadata": {},
   "source": [
    "First, let's select a random subset of 100 images in `/data_dev` to treat as our local test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354b9d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.022030Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_images = train_meta.sample(100, random_state=1).index.tolist()\n",
    "test_images[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1cc7c",
   "metadata": {},
   "source": [
    "Next, we'll move these files into the `/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b52e5f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.024721Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dest_dir = DATA_DIRECTORY / \"images\"\n",
    "if not dest_dir.exists():\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for img in test_images:\n",
    "    shutil.copy2(IMAGES_DIRECTORY / f\"{img}.png\", dest_dir / f\"{img}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86d5eb",
   "metadata": {},
   "source": [
    "Next we'll add a \"submission format\" CSV file to `/data`.\n",
    "\n",
    "> Note: A submission format file in this case is simply a CSV file that has the correct column and row indices required for a valid submission. Using this type of standard file is a useful way to ensure that all participants understand how their submission needs to be formatted in order to be accepted, and we tend to use these in most of our challenges.\n",
    "\n",
    "A submission format file for the full training set should already be present in `/data_dev`. We'll make a copy of this that only includes rows pertaining to the images in our local test set, and save it in `/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb9fec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.027346Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(DEV_DIRECTORY / \"submission_format.csv\", index_col=\"image_id\")\n",
    "submission_format_val = submission_format.loc[test_images]\n",
    "submission_format_val.to_csv(DATA_DIRECTORY / \"submission_format.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24faf50a21241f06",
   "metadata": {},
   "source": [
    "Let's also create a version of the labels file that only includes rows pertaining to our local test set. We can use this later for local scoring (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc0ca3ba63cd0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:10.035717Z",
     "start_time": "2024-03-05T22:20:10.029364Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(DEV_DIRECTORY / \"train_labels.csv\", index_col=\"image_id\")\n",
    "test_labels = train_labels.loc[test_images]\n",
    "test_labels.to_csv(DATA_DIRECTORY / \"test_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67a740-bd5e-4343-b196-bd3b7dc72a2f",
   "metadata": {},
   "source": [
    "### Download a pretrained model\n",
    "\n",
    "For this benchmark example, we will rely on YOLO, a commonly used algorithm for object detection. This will suit our purposes for demonstrating a very basic baseline approach, but you should explore more sophisticated methods yourself.\n",
    "\n",
    "To download the latest v8 YOLO model, we'll use the `ultralytics` library. The code below will download the model and move it into the `example_benchmark` directory, where we're assembling the components of our submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd86a95",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.032116Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "shutil.move('yolov8n.pt', PROJ_DIRECTORY / \"example_benchmark\" / \"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695a5ce",
   "metadata": {},
   "source": [
    "### Review the benchmark submission scripts\n",
    "\n",
    "Now let's take a look at the 2 files in `/example_benchmark`. This is the directory we are going to convert into a `submission.zip` file, which you can submit for the challenge.\n",
    "* The `main.sh` shell script is a _required_ file for any submission to this challenge. Our code execution platform will run this script, and you can have it call other scripts and resources as needed for your submission.\n",
    "* The `main.py` Python script is called by `main.sh` in this benchmark example, and this is where the work of generating predictions actually happens. There is no requirement that you also use Python here, but that's the approach we've taken since it is such a common one. The `main.py` script will iterate through all the images in the `submission_format.csv` and generate predictions using YOLO. If YOLO doesn't return a prediction, we simply generate a bounding box for the center of the image.\n",
    "\n",
    "The `example_benchmark` directory should now contain the following files.  Note that we do need to include the `yolov8n.pt` file because the submission will have no internet access when running on our platform.\n",
    "```\n",
    "spacecraft-pose-object-detection-runtime/\n",
    "└── example_benchmark/\n",
    "    ├── main.py\n",
    "    ├── main.sh\n",
    "    └── yolov8n.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e52b73-76f9-4158-889d-5bf5a00304de",
   "metadata": {},
   "source": [
    "### Three commands to prepare your submission\n",
    "To run and test the benchmark example, you just need to execute the following 3 commands:\n",
    "\n",
    "1. [`make pull`](#make-pull)\n",
    "2. [`make pack-benchmark`](#make-pack-benchmark)\n",
    "3. [`make test-submission`](#make-test-submission)\n",
    "\n",
    "These are defined in the project `Makefile` [here](https://github.com/drivendataorg/spacecraft-pose-object-detection-runtime/blob/main/Makefile). We'll walk through what each one does now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7777c9-1f41-42b0-a56c-b8e5eb91424c",
   "metadata": {},
   "source": [
    "<a id=\"make-pull\"></a>\n",
    "\n",
    "#### **`make pull`**\n",
    "\n",
    "To ensure that all participants are using the same runtime environment, we have a publicly accessible docker image hosted on [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/).\n",
    "\n",
    "The `make pull` command pulls the official version of the docker image and stores it locally. Having a local version of the competition image allows you to test your submission using the same image that is used during code execution.\n",
    "\n",
    "> **Note:** This command can take a little while to run the first time you pull the image. But after that it will be relatively quick since you'll have all the layers cached locally. You don't need to pull the image again each time you test your submission, unless the image has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd20d1-61a1-4445-ae99-066fc67d0ec5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.034143Z"
    }
   },
   "outputs": [],
   "source": [
    "!cd {PROJ_DIRECTORY} && make pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28310df2-9d17-41d0-a2cb-2abcb7441108",
   "metadata": {},
   "source": [
    "You should now have a local copy of the docker image, which you can verify by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4ed6a-3867-43a1-ade4-57c83d84a451",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.035471Z"
    }
   },
   "outputs": [],
   "source": [
    "!docker images | grep spacecraft-pose-object-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5aea2-cafd-4862-b1d9-e879761b6bb5",
   "metadata": {},
   "source": [
    "<a id=\"make-pack-benchmark\"></a>\n",
    "\n",
    "#### **`make pack-benchmark`** \n",
    "This command simply goes to your `example_benchmark` directory, zips the contents, and writes the zip archive to `submission/submission.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c8ed1-62ee-42a8-aa09-3c47e23d4699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T22:20:10.125941Z",
     "start_time": "2024-03-05T22:20:10.036983Z"
    }
   },
   "outputs": [],
   "source": [
    "!cd {PROJ_DIRECTORY} && make pack-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6106a9a-82a1-4ff9-b518-51945f536e8b",
   "metadata": {},
   "source": [
    "> **Note:** The `make pack-benchmark` command will check to see if you already have a `submission/submission.zip` and error if you do, so as not to overwrite existing work. If you already have this file, you'll need to manually remove it before running the command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e463f-206e-4482-bc1a-0f102216d6fc",
   "metadata": {},
   "source": [
    "After running the above command, we should now have a new **`submission/submission.zip`**.\n",
    "```\n",
    "spacecraft-pose-object-detection-runtime/\n",
    "├── benchmark_src/\n",
    "│   ├── main.py\n",
    "│   ├── main.sh\n",
    "│   └── yolov8n.pt\n",
    "└── submission/\n",
    "    └── submission.zip   <---- new file, this is what gets submitted on platform\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45552e-ba9a-4eea-80ea-3fb4c8476a63",
   "metadata": {},
   "source": [
    "This is the file that we will eventually upload to the competition platform for code execution. But before doing that, we want to test it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173b208-5120-4f16-9d12-5a22d3027ba2",
   "metadata": {},
   "source": [
    "<a id=\"make-test-submission\"></a>\n",
    "\n",
    "#### **`make test-submission`** \n",
    "This command simulates what happens during actual code execution, launching an instance of the official Docker image and running the same inference process that runs on the competition platform. The required host directories are mounted on the container, and the entrypoint script `main.sh` is executed. Note that when testing locally the contents of your local `data/` directory will be mounted on the container, whereas when your submission is running on our platform, the unseen test set will be mounted as `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6baaf4-46dc-4230-a69c-ab444f64e443",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T22:20:10.038754Z"
    }
   },
   "outputs": [],
   "source": [
    "!cd {PROJ_DIRECTORY} && make test-submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601f5a99b889451",
   "metadata": {},
   "source": [
    "Once the test run has completed, we should now have a new file with our predictions at **`submission/submission.csv`**.\n",
    "```\n",
    "spacecraft-pose-object-detection-runtime/\n",
    "├── benchmark_src/\n",
    "│   ├── main.py\n",
    "│   ├── main.sh\n",
    "│   └── yolov8n.pt\n",
    "└── submission/\n",
    "    ├── submission.zip   <---- this is what gets submitted on platform\n",
    "    └── submission.csv   <---- new file, predictions on test set\n",
    "```\n",
    "We also provide a scoring script that computes your score using the same calculation that's used for the public leaderboard. You can generate a score for your local testing with a command like the one below. Remember that this score will be computed on your local test set, and your score on the public leaderboard will be based on an unseen test set.\n",
    "```\n",
    "python scripts/score.py submission/submission.csv data/test_labels.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825a7d6-d2ff-4741-af19-cc4332f688ee",
   "metadata": {},
   "source": [
    "### Submitting to the platform\n",
    "We're almost done. Assuming that our test run completed and the `submission.csv` looks correct, it's time to submit the code on the platform.\n",
    "\n",
    "* Go to the [competition submissions page](https://www.drivendata.org/competitions/260/spacecraft-detection/submissions/) and upload your `submission/submission.zip`.\n",
    "* Please be patient while your submission is uploaded and executed. Your job may be queued if other jobs are still pending.\n",
    "* You can track the status of your submission on the [Code Execution Status](https://www.drivendata.org/competitions/260/submissions/code/) page. Logs will become available once the submission begins processing. To see them click on \"View Log\".\n",
    "\n",
    "Once your submission has been successfully uploaded, you will see something like this on the [Code Execution Status](https://www.drivendata.org/competitions/260/submissions/code/) page:\n",
    "\n",
    "![code execution status](https://drivendata-public-assets.s3.amazonaws.com/spacecraft-benchmark-code-status.jpg)\n",
    "\n",
    "Please be patient while your code is running. You may want to follow the links to check the logs for your job, which are live updated as your code job progresses.\n",
    "\n",
    "Once your job has completed, head over to the [Submissions](https://www.drivendata.org/competitions/260/spacecraft-detection/submissions/) page where you should be able to see your score. It will look something like this, except that we're sure you can do better than the benchmark!\n",
    "\n",
    "![score](https://drivendata-public-assets.s3.amazonaws.com/spacecraft-benchmark-score.jpg)\n",
    "\n",
    "**That's it! You're on your way to creating your own code submission!**\n",
    "\n",
    "**Head over to the [competition](https://www.drivendata.org/competitions/260/spacecraft-detection/) homepage to get started. And have fun! We can't wait to see what you build!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e32c-7f87-456a-971c-b060210d046a",
   "metadata": {},
   "source": [
    "_Images courtesy of NASA._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37ad15e07133d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T01:59:21.647463Z",
     "start_time": "2024-03-06T01:59:21.642153Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wget https://space-pose.drivendata-storage.org/object_detection/{0,1,2,3,4,5,6,7,8,9,a,b,c,d,e,f}.tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c45ef35de76d50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T01:59:45.346138Z",
     "start_time": "2024-03-06T01:59:45.324373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for char in {0..9} {a..f}; do\n",
    "    wget \"https://space-pose.drivendata-storage.org/object_detection/$char.tar\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce387b85397d5b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
